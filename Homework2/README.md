Name: Nina Baculinao
Uni: nb2406

COMS 4705 Natural Language Processing
Assignment 2 Readme

---------------------------------------------------------------------
1) Dependency Graphs

a. The visualized dependency graph of a sentence from each of the English, Danish, Korean, and Swedish training data sets can be found in the Homework 2 directory.

b. According to Nivre in "Algorithms for Deterministic Incremental
Dependency Parsing"" dependency graph G = (V, A) is projective if and only if, for every arc (i, l, j) ∈ A and node k ∈ V, if i < k < j or j < k < i then there is a subset of arcs {(i, l1, i1), (i1, l2, i2), ... (ik−1, lk, ik )} ∈ A such that ik = k. In plain English, none of the arcs should cross each other in a projective graph, i.e. if there exists an arc (i, l, j) where w_i is the dependent word and w_j is the head word, then there shouldn't exist a) any arcs with a dependent word less than w_i and head word less than w_j, because their arcs would intersect, or b) any arcs with a dependent word greater than w_i and head word greater than w_j because these conditions would create intersection of the arcs. Our model is only trained on projective sentences as the unconstrained problem of parsing non-projective sentences is much harder.

Some of these training sentences do not have projective dependency graphs (about 10% of the Swedish data set, for example). Therefore, we use a function providedcode/transition.py to determine if a dependency graph is projective.

    @staticmethod
    def _is_projective(depgraph):
        """
        Checks if a dependency graph is projective
        """
        arc_list = set()
        for key in depgraph.nodes:
            node = depgraph.nodes[key]
            if 'head' in node:
                childIdx = node['address'] // j for dependent wj
                parentIdx = node['head'] // i for head wi
                arc_list.add((parentIdx, childIdx)) // add (i, j)

        for (parentIdx, childIdx) in arc_list:
            # Ensure that childIdx < parentIdx
            if childIdx > parentIdx: // swap if j > i so j < i
                temp = childIdx
                childIdx = parentIdx
                parentIdx = temp
            for k in range(childIdx + 1, parentIdx): // for any k bt j...i
                for m in range(len(depgraph.nodes)): // for any node m
                    if (m < childIdx) or (m > parentIdx): // any m < j and > i
                        if (k, m) in arc_list: // this arc crosses
                            return False
                        if (m, k) in arc_list:
                            return False
        # print arc_list
        return True
        
Discussion: To check if a dependency graph of a sentence is projective, we first create a unique set of all the arcs in the dependency graph. This list is generated by traversing all the nodes in the dependency graph and adding the tuple (i, j) to the arc list if the node has an identified head, and i, j represent the node's head (parentIdx) and node's address (childIdx). After we have the completed arc_list, we traverse every (i, j) pairing, swapping i with j if j > i so that j < i, i.e. the childIdx is always less than the parentIdx. After that, we check all the nodes k in the range between the i and j (parent and child) indexes, and all the nodes m  > i and m < j (outside the parent and child indexes), to check if there exist any arcs in the arc list that have the tuple identity (k, m) or (m, k). If there exist any arcs like that, then the graph is not projective, because that arc from k to m or m to k would intersect with the arc from i to j.

c. Projective and non-projective sentence examples:

"She ate icecream with joy." (projective)

This sentence is projective because the verb "ate" has three dependencies, "she", "icecream" and "with joy" (where joy modifies with). None of the connecting arcs cross paths, so this is projective.

"You shouldn't put icecream into the freezer that is already melted." (non-projective)

The above sentence is non-projective because the prepositional phrase "into the freezer" is part of the complement of the verb "put", while the relative clause "that is already melted" modifies the noun "icecream." So the arc connecting "put" to "into" will cross the arc connecting "icecream" and "that is already melted." 

---------------------------------------------------------------------
2) Manipulating Configurations

a. The four operations left_arc, right_arc, shift and reduce are fully implemented in transition.py as a key part of the Nivre dependency parser is manipulating the parser configuration.

b. The bad features model only contains the following features for extraction:

- Stack[0] word, features and left and rightmost dependencies
- Buffer[0] word, features and left and rightmost dependencies

The performance of parser using the provided badfeatures.model is, as can be expected, quite bad. These features extracted in the feature model are not very informative as they are, especially as a word is much less informative to possible dependencies than the word's part of speech (ctag and tag attributes in the list of features)

The standard metrics unlabeled attachment score and labeled attachment score (the former assesses whether the output has the correct head and dependency arcs, while the latter additionally measures the accuracy of the labels of each arc) are all below 25% and practically 0% LAS accuracy for English, Danish and Korean. 


Swedish:
UAS: 0.229038040231
LAS: 0.125473013344

English:
UAS: 0.0518518518519 
LAS: 0.0

Danish:
UAS: 0.123552894212 
LAS: 0.00718562874251

Korean:
UAS: 0.115488605639 
LAS: 0.0

---------------------------------------------------------------------
3) Dependency Parsing

Code format | CoNLL format| Description
------------|-------------|-------------
address     | ID          | Integer token identifier
word        | FORM        | Word or punctuation
lemma       | LEMMA       | Stem of token
ctag        | CPOSTAG     | Coarse-grained POS tag
tag         | POSTAG      | Fine-grained POS tag
feats       | FEATS       | Additional syntactic features
head        | HEAD        | Token's head
rel         | DEPREL      | Token's dependency relation to its head
deps        | -           | Dictionary of token's dependencies by their POS : ID
-           | PHEAD       | Projective head of token
-           | PDEPREL     | and its dependency relation

Example data from a token in our implementation:

```
{u'ctag': u'VERB',
u'head': 4,
u'word': u'achieve',
u'rel': u'ccomp',
u'lemma': u'_',
u'tag': u'VB',
u'deps': defaultdict(<type 'list'>, {u'p': [12], u'advmod': [9], u'adpmod': [13], u'dobj': [7]}),
u'address': 5,
u'feats': u'_'}
```

Final feature model:

Two target token + one lookahead token

address      | word | lemma | ctag | tag | feats | head (word) | rel | ldep, rdep
------------:|:----:|:-----:|:----:|:---:|:-----:|:-----------:|:---:|:----------:
STK[0]       |  +   |   +   |  -   |  +  |   +   |      -      |  +  |    +   +
STK[1]       |      |       |  +   |  +  |       |             |     |
BUF[0]       |  +   |   +   |  +   |  +  |   +   |      -      |  +  |    +   +
BUF[1]       |  +   |   +   |  +   |  +  |   +   |             |     |
BUF[2]       |      |       |  +   |  +  |       |             |     |
BUF[3]       |      |       |  +   |  +  |       |             |     |

a. Edit featureextractor.py and try to improve the performance of your feature extractor! The features that we have provided are not particularly effective, but adding a few more can go a long way. Add at least three feature types and describe their implementation, complexity, and performance in your README.txt. It may be helpful to take a look at the book Dependency Parsing4 by Kubler, McDonald, and Nivre for some ideas.

b. Generate trained models for English, Danish, Swedish, and Korean data sets, and save the trained model for later evaluation.

c. Score your models against the test data sets for Danish, Swedish, and Korean. You should see dramatically improved results as compared to before, around 70% or better for both LAS and UAS with 200 training sentences.

/home/coms4705/Documents/Homework2/data/english/train/en-universal-train.conll

from providedcode import dataset

testdata = dataset.get_english_dev_corpus().parsed_sents()


from dependencycorpusreader import DependencyCorpusReader
import os

BASE_PATH = './'


def get_english_dev_corpus():
    root = os.path.join(BASE_PATH,)
    files = ['dev/en-universal-dev.conll']
    return DependencyCorpusReader(root, files)        



    test(english.model): train english model with TRAIN instead of dev data
     Number of training examples : 200
     Number of valid (projective) examples : 200
    Training support vector machine...
    done!
    UAS: 0.837037037037
    LAS: 0.834567901235
    
    test(swedish.model): add swedish model el and conll data
     Number of training examples : 200
     Number of valid (projective) examples : 180
    Training support vector machine...
    done!
    UAS: 0.886476797451
    LAS: 0.872137024497
    
    test(danish.model): add danish model and conll data to goodybag
     Number of training examples : 200
     Number of valid (projective) examples : 174
    Training support vector machine...
    done!
    UAS: 0.862874251497
    LAS: 0.852095808383
    
    test(korean.model): add korean model and conll data to goodybag
    UAS: 0.853997682503
    LAS: 0.852838933951
    
    real0m40.103s
    user0m36.330s
    sys0m0.844s


Complexity in terms of length of sentence and size of feature space

shift-reduce parser

d. In your README.txt file, discuss in a few sentences the complexity of the arc-eager shift-reduce parser, and what tradeoffs it makes.

---------------------------------------------------------------------
4) Parser executable


a. Create a file parse.py which can be called as follows:

```
cat englishfile | python parse.py english.model > englishfile.conll 
```
b. The standard input of the program will be the sentences to be parsed, separated by line. The expected standard output is a valid CoNLL-format file which can be viewed by the MaltEval evaluator, complete with HEAD and REL columns. The first argument should be the path to the trained model file created in the previous step.

c. Sample output for englishfile is not directly provided, as your model may have been trained on different features. However, we do expect that the CoNLL output is valid and contains a projective dependency graph for each sentence.

---------------------------------------------------------------------
DELIVERABLES

1. Dependency graph plots
    a. $HW2_ROOT/figure_en.png - image of a sentence from English training data
    b. $HW2_ROOT/figure_da.png - image of a sentence from Danish training data
    c. $HW2_ROOT/figure_ko.png - image of a sentence from Korean training data
    d. $HW2_ROOT/figure_sw.png - image of a sentence from Swedish training data
2. Transitions between configurations
    a. $HW2_ROOT/transition.py
3. Feature extractor
    a. $HW2_ROOT/featureextractor.py
4. Trained model files
    a. $HW2_ROOT/english.model - trained TransitionParser for English
    b. $HW2_ROOT/danish.model - trained TransitionParser for Danish    
    c. $HW2_ROOT/korean.model - trained TransitionParser for Korean
    d. $HW2_ROOT/swedish.model - trained TransitionParser for Swedish
5. Standard parser
    a. $HW2_ROOT/parse.py - parser program
6. README file containing results and discussions
    a. $HW_ROOT/README.txt

You may note that we have not provided sample lines or output. This is because you are only being asked to turn in the trained model files, the images, the edited code, and a readme file. For evaluation, we will be running your code on the English test dataset and examining your performance, as well as looking at how your parser performs on the other languages. Please ensure that your model can be loaded with TransitionParser.load


A

/\

B C (ABC)


cky parser vs shift reduce parser
